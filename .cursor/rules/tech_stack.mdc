### Scala Best Practices
- **Immutability:** Favor `val` over `var` and use immutable collections (e.g., `List`, `Vector`) to prevent side effects.
- **Case Classes for Data:** Use `case class`es to model the schema of our data. This makes the code type-safe and easier to read when working with Spark Datasets.
- **Functional Style:** Employ functional constructs like `map`, `filter`, and `Option` to handle data transformations and null values gracefully.

### Spark MLlib Best Practices
- **DataFrame API:** Use the DataFrame API for all data manipulation. It is optimized by Spark's Catalyst optimizer and is the standard for modern Spark applications.
- **ML Pipelines:** Structure the entire workflow, from data preprocessing to model training and evaluation, within an ML `Pipeline`. This makes the code modular, reusable, and less prone to errors.
- **Lazy Evaluation:** Always remember that transformations in Spark are lazy. Actions (like `show()`, `count()`, or `write()`) are required to trigger the computation.
- **Schema Definition:** Define an explicit schema when reading data instead of relying on schema inference. This improves performance and prevents errors from incorrect data types.
description:
globs:
alwaysApply: false
---
